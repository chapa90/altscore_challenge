{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Galactic Empire Cost of Living Prediction**\n",
    "\n",
    "### **Your Mission**\n",
    "As a newly recruited Rebel intelligence officer, you’ve been granted access to highly classified mobility data intercepted from the Empire’s network. Your task is to analyze this information, predicting the true cost of living for regions across the galaxy. Each prediction could be a turning point in the struggle against Imperial oppression.\n",
    "\n",
    "The data is encrypted using the Galactic Council's powerful H3 geospatial grid—an advanced system known for its precision. With the clock ticking, you must decode the movement patterns, extract insights, and accurately estimate the cost of living in each sector before the Empire discovers your mission.\n",
    "\n",
    "Only the most skilled and insightful intelligence officers will succeed. The competition is fierce, and only those who can wield the power of data with precision and creativity will prevail.\n",
    "\n",
    "### **The Tools of Rebellion**\n",
    "The Rebellion’s intelligence arm has provided you with a set of encrypted data files. These are the tools you’ll need to expose the Empire’s deception:\n",
    "\n",
    "- **train.csv**: Data from sectors already liberated by the Rebellion. Each entry contains precise coordinates and the true cost of living, encrypted by the Galactic Council's H3 technology.\n",
    "- **test.csv**: Sectors still under Imperial control, awaiting your predictions to reveal the hidden cost of living.\n",
    "- **mobility.parquet**: A data set containing the secret movement patterns of operatives, smugglers, and civilians across the galaxy. These movements hold the clues to understanding the local economies of each region.\n",
    "\n",
    "### **The Power of Prediction**\n",
    "The Rebellion isn't just looking for predictions—they’re seeking insights. Your mission is not only to estimate the cost of living but to transform chaos into clarity. The Empire’s secrets are buried in raw data, waiting for you to uncover the truth. Use creativity, strategic feature engineering, and any additional data you can find to strengthen the Rebellion’s cause.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data processing, geospatial conversion, and machine learning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h3\n",
    "import geopandas as gpd\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load and Explore Train and Test Data\n",
    "# Define the dataset path\n",
    "DATA_PATH = r\"\\alt-score-data-science-competition\"\n",
    "\n",
    "# Load training data (contains hex_id and cost_of_living values)\n",
    "train = pd.read_csv(f\"{DATA_PATH}\\\\train.csv\")\n",
    "\n",
    "# Load test data (contains hex_id values for prediction)\n",
    "test = pd.read_csv(f\"{DATA_PATH}\\\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution of train/test hex_id: 8\n"
     ]
    }
   ],
   "source": [
    "# Check resolution of hex_id in train and test data to ensure consistency\n",
    "example_hex = train[\"hex_id\"].iloc[0]\n",
    "desired_resolution = h3.get_resolution(example_hex)\n",
    "print(f\"Resolution of train/test hex_id: {desired_resolution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Process Mobility Data in Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load large mobility data file in chunks to save memory\n",
    "mobility = pq.ParquetFile(f\"{DATA_PATH}\\\\mobility_data.parquet\")\n",
    "chunk_size = 500_000    # Number of rows to process at a time\n",
    "\n",
    "# Dictionary to store aggregated features for each hex_id\n",
    "hex_features = {}\n",
    "\n",
    "for batch in mobility.iter_batches(batch_size=chunk_size):\n",
    "    # Convert batch to a pandas DataFrame\n",
    "    chunk_df = batch.to_pandas()\n",
    "    \n",
    "    # Convert lat/lon to H3 hexagons with matching resolution\n",
    "    chunk_df[\"hex_id\"] = chunk_df.apply(lambda row: h3.latlng_to_cell(row[\"lat\"], row[\"lon\"], desired_resolution), axis=1)\n",
    "    \n",
    "    # Aggregate features per hex_id\n",
    "    grouped = chunk_df.groupby(\"hex_id\").agg(\n",
    "        device_count=(\"device_id\", \"nunique\"),  # Unique devices per hex\n",
    "        visit_count=(\"device_id\", \"count\"),      # Total visits per hex\n",
    "        avg_time=(\"timestamp\", \"mean\")           # Average timestamp per hex\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Update the hex_features dictionary with aggregated data\n",
    "    for _, row in grouped.iterrows():\n",
    "        hex_id = row[\"hex_id\"]\n",
    "        if hex_id not in hex_features:\n",
    "            hex_features[hex_id] = row[1:].values\n",
    "        else:\n",
    "            hex_features[hex_id] += row[1:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Convert Aggregated Features to DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature dictionary to DataFrame\n",
    "mobility_features_df = pd.DataFrame.from_dict(hex_features, orient='index', columns=[\"device_count\", \"visit_count\", \"avg_time\"]).reset_index()\n",
    "mobility_features_df.rename(columns={\"index\": \"hex_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Debugging Hex ID Mismatches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hex IDs missing in mobility features (train): 124\n",
      "Hex IDs missing in mobility features (test): 155\n"
     ]
    }
   ],
   "source": [
    "# Compare unique hex_id values across train, test, and mobility datasets\n",
    "train_hex_ids = set(train[\"hex_id\"].unique())\n",
    "test_hex_ids = set(test[\"hex_id\"].unique())\n",
    "mobility_hex_ids = set(mobility_features_df[\"hex_id\"].unique())\n",
    "\n",
    "# Identify missing hex_ids\n",
    "missing_train_hex = train_hex_ids - mobility_hex_ids\n",
    "missing_test_hex = test_hex_ids - mobility_hex_ids\n",
    "print(f\"Hex IDs missing in mobility features (train): {len(missing_train_hex)}\")\n",
    "print(f\"Hex IDs missing in mobility features (test): {len(missing_test_hex)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Merge Train Data with Mobility Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join train data with mobility features and fill missing values with 0\n",
    "train_merged = train.merge(mobility_features_df, on=\"hex_id\", how=\"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train Model using Random Forest Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y) variablesl\n",
    "X = train_merged.drop(columns=[\"cost_of_living\", \"hex_id\"])\n",
    "y = train_merged[\"cost_of_living\"]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_pred = model_rf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 0.044339250462866006\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Squared Error\n",
    "mse_rf = mean_squared_error(y_val, y_pred)\n",
    "print(f\"Validation MSE: {mse_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Make Predictions on Test Data from RandomForestRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join test data with mobility features and fill missing values with 0\n",
    "test_merged = test.merge(mobility_features_df, on=\"hex_id\", how=\"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure test_X has the same features as X_train\n",
    "test_X = test_merged.drop(columns=[\"hex_id\"], errors='ignore')\n",
    "test_X = test_X.reindex(columns=X_train.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the test data\n",
    "test[\"cost_of_living\"] = model_rf.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Save Predictions to CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved\n"
     ]
    }
   ],
   "source": [
    "# Save the predictions to a submission file\n",
    "test.to_csv(\"submission_rf.csv\", index=False)\n",
    "print(\"Predictions saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train Model using Random Forest Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import corresponding library\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Separate features (X) and target (y) variablesl\n",
    "X = train_merged.drop(columns=[\"cost_of_living\", \"hex_id\"])\n",
    "y = train_merged[\"cost_of_living\"]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an XGBoost Regressor\n",
    "model_xg = XGBRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "model_xg.fit(X_train, y_train)\n",
    "y_pred = model_xg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 0.05399483775948132\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Squared Error\n",
    "mse_xg = mean_squared_error(y_val, y_pred)\n",
    "print(f\"Validation MSE: {mse_xg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Make Predictions on Test Data from XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join test data with mobility features and fill missing values with 0\n",
    "test_merged = test.merge(mobility_features_df, on=\"hex_id\", how=\"left\").fillna(0)\n",
    "\n",
    "# Ensure test_X has the same features as X_train\n",
    "test_X = test_merged.drop(columns=[\"hex_id\"], errors='ignore')\n",
    "test_X = test_X.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Generate predictions for the test data\n",
    "test[\"cost_of_living\"] = model_xg.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Save Predictions to CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved\n"
     ]
    }
   ],
   "source": [
    "# Save the predictions to a submission file\n",
    "test.to_csv(\"submission_xg.csv\", index=False)\n",
    "print(\"Predictions saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
